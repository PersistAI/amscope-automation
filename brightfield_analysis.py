# -*- coding: utf-8 -*-
"""DeployedModel_SEMAnalyst Brightfield_2024Jan03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z7Dc1FzW-L6RCKPIc51ZGcubXAjznNy1

#**Parameters**

This is for DevOps, and is to be hard coded. The paths under this section are the only variables that need to be set by the DevOps team, with the exception of the data upload and model upload. Once these paths are set, assuming that the data and model are correct, everything will run as expected.

There is no need to continuously upload the model upon every run once the best fit model is uploaded once and necessary libraries are installed. And the data upload path can be customized / optimized by DevOps to intergrate this workflow into production / the webpage.
"""

main_folder_path = "/content/data" # where the magic (pre-processing, post-processing, and export) happens
model_dir = "/content/persist_model" # location for the best fit (best.pt) PyTorch model to be uploaded (only once, then use until update / redo tuning)

# General Lib
import os
import re               # Regex
import math             # Math for calculations
# import utils
import shutil                     # For moving and deleting files / folders
import random                     # For randomly splitting the files
import pandas as pd               # Data manipulation and analysis
import matplotlib.pyplot as plt  # Data visualization
import matplotlib.image as mpimg   # Data visualization for pngs and jpegs
import subprocess
import cv2
import numpy as np

# Data management libraries
import IPython
from IPython.display import Image, clear_output  # to display images
import zipfile
from zipfile import ZipFile

# For making ~fancy pantsy~ excel data products
from openpyxl import Workbook
from openpyxl.utils import get_column_letter
from openpyxl import load_workbook
from openpyxl.drawing.image import Image

# Run deployed model
import torch
import torchvision
import ultralytics
from ultralytics.utils import TryExcept
# import onnx
# import onnxruntime
# from onnx import numpy_helper

"""## Additional output paths"""

# Define source and target directories
postprocessing_imgs_path = os.path.join(main_folder_path,'dat_folder/output/postprocessed_imgs')
imgs_output_dir = os.path.join(main_folder_path,'output/imgs')

# Define the path for the zip file for the final output
zip_file_path = os.path.join(main_folder_path,'output.zip')

"""# **Custom Functions and Helper Functions**"""

# Prepares data for inference - turns raw into CLAHE images
def postprocess_image_enhanced(image_path):
    # Load the image
    image = cv2.imread(image_path)

    # Convert the image to grayscale for CLAHE
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced_gray = clahe.apply(gray)

    # Convert the enhanced grayscale image back to BGR
    enhanced_bgr = cv2.cvtColor(enhanced_gray, cv2.COLOR_GRAY2BGR)

    # Extract the directory from the image path
    directory = os.path.dirname(image_path)
    output_path = os.path.join(directory, "processed_image_enhanced.jpg")

    # Save the postprocessed image to the same directory
    cv2.imwrite(output_path, enhanced_bgr)
    return output_path

def apply_bounding_boxes(original_path, enhanced_path, output_folder):
    # Load the original and enhanced images
    original = cv2.imread(original_path)
    enhanced = cv2.imread(enhanced_path)

    # Ensure enhanced image is in BGR format (3 channels)
    if len(enhanced.shape) == 2:  # grayscale image
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)

    # Make sure both images are of the same size
    if original.shape[:2] != enhanced.shape[:2]:
        enhanced = cv2.resize(enhanced, (original.shape[1], original.shape[0]))

    # Convert the original image to HSV color space
    hsv = cv2.cvtColor(original, cv2.COLOR_BGR2HSV)

    # Mask the red bounding boxes from the original image
    lower_red = np.array([0, 120, 70])
    upper_red = np.array([10, 255, 255])
    mask1 = cv2.inRange(hsv, lower_red, upper_red)

    lower_red = np.array([170, 120, 70])
    upper_red = np.array([180, 255, 255])
    mask2 = cv2.inRange(hsv, lower_red, upper_red)

    mask = mask1 + mask2
    red_bboxes = cv2.bitwise_and(original, original, mask=mask)

    # Apply the red bounding boxes to the enhanced image
    final_image = cv2.addWeighted(enhanced, 1, red_bboxes, 1, 0)

    # Save the resulting image
    final_path = os.path.join(output_folder, os.path.basename(original_path))
    cv2.imwrite(final_path, final_image)

    return final_path

def process_image(image_path, output_folder):
    enhanced_path = postprocess_image_enhanced(image_path)
    final_image_path = apply_bounding_boxes(image_path, enhanced_path, output_folder)
    return final_image_path

def CLAHE_images(parent_path):
    # Create a subfolder for the enhanced images
    output_folder = os.path.join(parent_path, "CLAHE_images")
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Process each image in the specified folder
    for filename in os.listdir(parent_path):
        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Add more extensions if needed
            image_path = os.path.join(parent_path, filename)
            process_image(image_path, output_folder)

# Created new function to check if best.pt file exists - 2023Dec19 JMM
# If we want to use a different name than "best.pt," I would suggest setting a global variable - JMM
def check_and_upload_model(path):
    # Check if the path exists
    if not os.path.exists(path):
        # If the path doesn't exist, create it
        os.makedirs(path, exist_ok=True)
        print(f"Folder '{path}' created. Please upload 'best.pt'.")
    else:
        print(f"Folder '{path}' already exists.")

    # Define the full path for 'best.pt' in the target directory
    pt_file_path = os.path.join(path, 'best.pt')

    # Check if the PT file exists within the path
    if not os.path.exists(pt_file_path):
        print("Please upload 'best.pt' file.")
        uploaded = files.upload()  # Request file upload

        # Move the uploaded file to the specified path
        for filename in uploaded.keys():
            # Source path (current directory)
            src = filename
            # Destination path (specified path)
            dst = pt_file_path

            # Move the file
            shutil.move(src, dst)
            print(f"'{filename}' uploaded and moved to '{dst}'.")

    else:
        print("'best.pt' file already exists. Proceeding with the process.")

def load_images_from_folder(folder):
  images = []
  filenames = []
  for filename in os.listdir(folder):
      if filename.endswith((".png", ".jpg", ".jpeg")):  # Add or remove file types as needed
          img_path = os.path.join(folder, filename)

          # Or, load using OpenCV and convert BGR to RGB
          img = cv2.imread(img_path)[:, :, ::-1]
          images.append(img)
          filenames.append(filename)

  return images, filenames

# Reinjecting the upload_and_extract_to_folder fun for validation of files

def upload_and_extract_to_folder(data_storage_path, folder_name):
    """
    Prompts the user to upload a zip file and extracts it to the specified folder.
    """
    print(f"Please upload the zip file for {folder_name} data:")
    uploaded = files.upload()

    for filename in uploaded.keys():
        print('User uploaded file "{name}" with length {length} bytes'.format(
            name=filename, length=len(uploaded[filename])))

        # Check if the uploaded file is a zip file
        if filename.endswith('.zip'):
            # target_folder = os.path.join(main_path, folder_name)
            target_folder = data_storage_path

            # Ensure the target folder exists
            if not os.path.exists(target_folder):
                os.makedirs(target_folder)

            # Extract the contents of the zip file to the target folder
            with zipfile.ZipFile(filename, 'r') as zip_ref:
                names = zip_ref.namelist()

                # Determine if there's a single top-level directory
                if len(names) > 1 and all(name.startswith(names[0]) for name in names):
                    common_prefix = names[0]
                else:
                    common_prefix = ""

                # Extract each file individually, skipping directories
                for name in names:
                    if not name.endswith('/'):  # Skip directories
                        destination_path = os.path.join(target_folder, name.replace(common_prefix, ""))
                        destination_dir = os.path.dirname(destination_path)

                        # Create directory structure if it doesn't exist
                        if not os.path.exists(destination_dir):
                            os.makedirs(destination_dir)

                        # Extract file
                        with zip_ref.open(name) as source, open(destination_path, "wb") as target:
                            target.write(source.read())

                print(f'Extracted contents of {filename} to {target_folder}/')

            # Optionally, you can list the extracted files
            for root, dirs, files_in_dir in os.walk(target_folder):
                for file_in_dir in files_in_dir:
                    print(os.path.join(root, file_in_dir))

"""## Post-processing statistics functions"""

# Function to calculate the diameter and area of the bounding box
def calculate_diameter_and_area(bbox, pixel_val=368):
    width_px = bbox[2] - bbox[0]
    height_px = bbox[3] - bbox[1]

    # Calculate the diameter in pixels
    diameter_px = math.sqrt(width_px ** 2 + height_px ** 2)

    # Convert pixels to micrometers using the given pixel to micrometer ratio
    pixels_per_um = pixel_val / 100  # Number of pixels per micrometer
    diameter_um = diameter_px / pixels_per_um

    # Calculate the area in square micrometers
    radius_um = diameter_um / 2
    area_um = math.pi * (radius_um ** 2)

    return round(diameter_um, 4), round(area_um, 4)

# Function to check if two boxes overlap by at least a certain percentage
def overlap_percentage(boxA, boxB):
    # Calculate intersection
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    # Compute the area of intersection rectangle
    interArea = max(0, xB - xA) * max(0, yB - yA)

    # Compute the area of both the prediction and ground-truth rectangles
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    # Compute the smallest area of the two boxes
    minArea = min(boxAArea, boxBArea)

    # Calculate the overlap percentage on the smallest box
    overlap = interArea / minArea

    return overlap

# Prepare images for export, apply mask back on image
def process_export_image(image_path, detections, output_folder):
    img = cv2.imread(image_path)
    diameters_areas = [(calculate_diameter_and_area(bbox[:4])) for bbox in detections]
    boxes_to_remove = set()

    # Determine boxes to remove based on overlap percentage
    for i in range(len(detections)):
        for j in range(i + 1, len(detections)):
            if overlap_percentage(detections[i], detections[j]):
                boxes_to_remove.add(i if diameters_areas[i][0] > diameters_areas[j][0] else j)

    bounding_boxes_data = []
    sequential_id = 1  # Start with ID 1

    for idx, (bbox, (diameter, area)) in enumerate(zip(detections, diameters_areas)):
        if idx not in boxes_to_remove:
            x_min, y_min, x_max, y_max, conf = map(int, bbox[:5])
            # Annotate the image with thicker rectangles and larger text
            cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 4)  # Thicker boundary
            cv2.putText(img, str(sequential_id), (x_min, y_min - 30), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 4)  # Larger text, white color

            # Format and add data string with the new sequential ID
            data_str = f"ID: {sequential_id}, Diameter: {diameter}, Area: {area}"
            bounding_boxes_data.append(data_str)
            sequential_id += 1  # Increment ID for the next box

    # Save annotated image
    img_name = os.path.splitext(os.path.basename(image_path))[0]
    annotated_path = os.path.join(output_folder, f"{img_name}_annotated.jpg")
    cv2.imwrite(annotated_path, img)

    # Save bounding box data to txt file
    txt_path = os.path.join(output_folder, f"boundingboxes_{img_name}.txt")
    with open(txt_path, 'w') as file:
        for data in bounding_boxes_data:
            file.write(data + '\n')

# Calculate image average statistics
def calculate_statistics(diameters_areas):
    remaining_diameters = [diam for diam, _ in diameters_areas]
    remaining_areas = [area for _, area in diameters_areas]

    total_particles = len(remaining_diameters)
    average_diameter = np.mean(remaining_diameters) if remaining_diameters else 0
    average_area = np.mean(remaining_areas) if remaining_areas else 0
    std_dev_diameter = np.std(remaining_diameters) if remaining_diameters else 0
    std_dev_area = np.std(remaining_areas) if remaining_areas else 0
    diameter_range = [np.min(remaining_diameters), np.max(remaining_diameters)] if remaining_diameters else [0, 0]
    area_range = [np.min(remaining_areas), np.max(remaining_areas)] if remaining_areas else [0, 0]

    stats_data = (
        f"Total number of particles: {total_particles}\n"
        f"Average diameter: {average_diameter:.4f}\n"
        f"Average area: {average_area:.4f}\n"
        f"Standard deviation (diameter): {std_dev_diameter:.4f}\n"
        f"Standard deviation (area): {std_dev_area:.4f}\n"
        f"Range for particle diameter [min,max]: {diameter_range}\n"
        f"Range for particle area [min,max]: {area_range}\n"
    )

    return stats_data

def calculate_and_update_statistics_in_txt_files(folder_path):
    # Loop through all files in the folder
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):
            txt_path = os.path.join(folder_path, filename)

            # Read the existing data from the file
            with open(txt_path, 'r') as file:
                lines = file.readlines()

            # Extract diameters and areas from the existing data
            diameters_areas = []
            for line in lines:
                parts = line.split(',')
                if len(parts) >= 3:
                    diameter = float(parts[1].split(':')[1].strip())
                    area = float(parts[2].split(':')[1].strip())
                    diameters_areas.append((diameter, area))

            # Calculate statistics
            stats_data = calculate_statistics(diameters_areas)

            # Write the statistics and the original data back to the file
            with open(txt_path, 'w') as file:
                file.write(stats_data + '\n' + ''.join(lines))

"""## Visualization functions"""

# Function to extract diameters
def extract_diameters_from_file(file_path):
    diameters = []
    with open(file_path, 'r') as file:
        for line in file:
            match = re.search(r"Diameter: (\d+\.\d+)", line)
            if match:
                diameters.append(float(match.group(1)))
    return diameters

# Function to plot the histogram of diameters
def plot_diameter_histogram(diameters, title, save_path):
    plt.figure(figsize=(8, 6))
    plt.hist(diameters, bins=20, color='green', alpha=0.7)
    plt.title(title)
    plt.xlabel('Diameter (Î¼m)') # NEED to make these units dynamic later to also accept nm - JMM 2024Jan05
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()

"""## Data wrangling and export functions"""

# Function to extract statistics from the text file for the 'img_statistics' sheet
def extract_statistics(file_path):
    stats = {
        "Total number of particles": "",
        "Average diameter": "",
        "Average area": "",
        "Standard deviation (diameter)": "",
        "Standard deviation (area)": "",
        "Range for particle diameter [min,max]": "",
        "Range for particle area [min,max]": ""
    }
    try:
        with open(file_path, 'r') as file:
            lines = file.readlines()
            for line in lines:
                if ':' in line and not line.strip().startswith('ID:'):
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    if key in stats:
                        if "Range" in key:
                            stats[key] = re.search(r'\[(.*?)\]', value).group(1)
                        else:
                            stats[key] = value
    except Exception as e:
        print(f"An error occurred while processing {file_path}: {e}")
    return stats

# Function to create DataFrame from extracted statistics for the 'img_statistics' sheet
def create_statistics_df(folder_path):
    data = []
    for filename in os.listdir(folder_path):
        if filename.startswith('boundingboxes_') and filename.endswith('.txt'):
            try:
                file_path = os.path.join(folder_path, filename)
                image_name = filename.replace('boundingboxes_', '').replace('.txt', '')
                stats = extract_statistics(file_path)
                stats['Image Name'] = image_name
                data.append(stats)
            except Exception as e:
                print(f"An error occurred with file {filename}: {e}")
    df = pd.DataFrame(data)
    cols = ['Image Name'] + [col for col in df if col != 'Image Name']
    df = df[cols]
    return df

# Function to parse the individual particle data from the text files for the 'individ_img_info' sheet
def parse_particle_data(file_path):
    particle_data = []
    with open(file_path, 'r') as file:
        lines = file.readlines()
        for line in lines:
            if line.strip().startswith('ID:'):
                parts = line.split(',')
                particle_id = int(re.search(r'ID: (\d+)', parts[0]).group(1))
                diameter = float(re.search(r'Diameter: ([\d.]+)', parts[1]).group(1))
                area = float(re.search(r'Area: ([\d.]+)', parts[2]).group(1))
                particle_data.append((particle_id, diameter, area))
    return particle_data

"""# **Prepare uploaded data for model**"""

# Upload dat_folder
data_storage_path = os.path.join(main_folder_path, "dat_folder")
upload_and_extract_to_folder(data_storage_path, "Brightfield particle image") # Will make this name dynamic / change it depending on data type uploaded - 2024Jan05 JMM

CLAHE_images(data_storage_path) # prepared data to be stored in "CLAHE_images" folder

# Import the model
check_and_upload_model(model_dir) # Use of new function - 2023Dec19 JMM

# Use the following model path, again change name or make dynamic - 2023Dec19 JMM
exported_model = model_dir+"/best.pt"

print(exported_model)

"""# **Annotate with Deployed Model - images *NOT* to be exported for end user**"""

# Model
# model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, etc.
model = torch.hub.load('ultralytics/yolov5', 'custom', exported_model)  # custom trained model

# With conf = 0.30
# image 1/8: 1944x2592 4 particles
# image 2/8: 1944x2592 5 particles
# image 3/8: 1944x2592 25 particles
# image 4/8: 1944x2592 62 particles
# image 5/8: 1944x2592 15 particles
# image 6/8: 1944x2592 7 particles
# image 7/8: 1944x2592 45 particles
# image 8/8: 1944x2592 10 particles

# With conf = 0.20 - choosing this as the threshold and using post-processing to get rid of overlapping particles
# image 1/8: 1944x2592 7 particles
# image 2/8: 1944x2592 7 particles
# image 3/8: 1944x2592 40 particles
# image 4/8: 1944x2592 93 particles
# image 5/8: 1944x2592 22 particles
# image 6/8: 1944x2592 10 particles
# image 7/8: 1944x2592 67 particles
# image 8/8: 1944x2592 14 particles

# Images
# Specify the folder path
CLAHE_folder_path = os.path.join(data_storage_path, "CLAHE_images")

# Load images from the specified folder and store filenames
image_list, image_filenames = load_images_from_folder(CLAHE_folder_path)
print(f"Loaded {len(image_list)} images from the folder")

# Adjust model params
model.conf = 0.20  # confidence threshold (0-1)
model.iou = 0.45  # NMS IoU threshold (0-1)

# Inference
results = model(image_list, size=320)  # includes NMS

# Results
results.print()  # print results to screen
# results.show()  # display results

# Render the images with annotations
annotated_imgs = results.render()

# Ensure the output directory exists
# output_dir = '/content/data/dat_folder/output/model_inference'
model_inf_dir = os.path.join(data_storage_path, 'output/model_inference')
os.makedirs(model_inf_dir, exist_ok=True)

# Save the images manually using OpenCV
for i, img in enumerate(annotated_imgs):
    # Extract original filename and add '_annotated' suffix
    original_filename = image_filenames[i]
    filename_without_extension = os.path.splitext(original_filename)[0]
    annotated_filename = filename_without_extension + '_annotated.jpg'

    # Define the save path
    save_path = os.path.join(model_inf_dir, annotated_filename)

    # Save the image
    cv2.imwrite(save_path, img)

# Data
# for i, img_detections in enumerate(results.xyxy):
#     print(f'\nImage {i+1} predictions:', img_detections)  # print predictions for each image

"""# **Post process annotations and prepare final data products for end user**"""

# Specify model path and folder paths
# data_folder = '/content/data/dat_folder/CLAHE_images' # I've made it dynamic now, should be good... 2024Jan04 JMM
output_folder = os.path.join(main_folder_path,'output')
output_folder_imgs = os.path.join(output_folder,'imgs')
postprocessed_imgs_path = os.path.join(data_storage_path,'output/postprocessed_imgs')


os.makedirs(output_folder, exist_ok=True)
os.makedirs(output_folder_imgs, exist_ok=True)
os.makedirs(postprocessed_imgs_path, exist_ok=True)

# Load model and images
model = torch.hub.load('ultralytics/yolov5', 'custom', path=exported_model)
image_list, image_filenames = load_images_from_folder(CLAHE_folder_path)
results = model(image_list, size=320)

# Process each image
for i, img_detections in enumerate(results.xyxy):
    process_export_image(os.path.join(CLAHE_folder_path, image_filenames[i]), img_detections.cpu().numpy(), postprocessed_imgs_path)

# process_stats_path = '/content/data/dat_folder/output/export_images'
calculate_and_update_statistics_in_txt_files(postprocessed_imgs_path)

# List to store all diameters from all files for the cumulative plot
all_diameters = []

# Loop through each file, plot histogram, and collect diameters
for file in os.listdir(postprocessed_imgs_path):
    if file.startswith('boundingboxes_') and file.endswith('.txt'):
        file_path = os.path.join(postprocessed_imgs_path, file)
        diameters = extract_diameters_from_file(file_path)
        all_diameters.extend(diameters)  # Add to cumulative list

        # Plot title
        base_name = file.replace('boundingboxes_', '').replace('.txt', '')
        title = f"{base_name} Microsphere Diameter Distribution Plot"
        save_path = os.path.join(postprocessed_imgs_path, f"{base_name}_diameter_distribution.png")
        plot_diameter_histogram(diameters, title, save_path)

# Plot the cumulative histogram
cumulative_title = "Cumulative Microsphere Diameter Distribution Plot"
cumulative_save_path = os.path.join(postprocessed_imgs_path, "cumulative_diameter_distribution.png")
plot_diameter_histogram(all_diameters, cumulative_title, cumulative_save_path)

# Return the path to the cumulative plot for reference
cumulative_save_path

# Create the main sheet that shows the overall statistics for each image
# Path where the .txt files are stored
postprocessed_imgs_path

# Path where the Excel file will be saved
output_excel_path = os.path.join(output_folder, 'SEManalyst_DataExport.xlsx')

df = create_statistics_df(postprocessed_imgs_path)

# # Path to save the Excel file
# excel_path = os.path.join(output_folder,'SEManalyst_DataExport.xlsx')

try:
    with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='img_statistics', index=False)
except Exception as e:
    print(f"An error occurred while saving the Excel file: {e}")

print(f"Excel file saved at {output_excel_path}")

# Save the DataFrame to an Excel file with adjusted column widths
with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
    df.to_excel(writer, sheet_name='img_statistics', index=False)

    # Get the workbook and sheet for formatting
    workbook = writer.book
    worksheet = writer.sheets['img_statistics']

    # Set column widths
    for col in worksheet.columns:
        max_length = max(len(str(cell.value)) for cell in col)
        adjusted_width = (max_length + 2) * 1.2
        worksheet.column_dimensions[get_column_letter(col[0].column)].width = adjusted_width

# The path to the Excel file is returned for your reference.
output_excel_path

# Add a second sheet where the distribution of diameters are saved

print(output_excel_path)

# Load the workbook
wb = load_workbook(output_excel_path)

# Add a new sheet named 'diameter_dist'
ws_img = wb.create_sheet('diameter_dist')

# Load the culmulative distrib img
img_path = cumulative_save_path
img = Image(img_path)

# Insert the image into the 'diameter_dist' sheet at cell A1
ws_img.add_image(img, 'A1')

# Save the workbook
wb.save(output_excel_path)

# Add a third sheet where the statistics are presented for each particle

# Load the existing workbook
wb = load_workbook(output_excel_path)

# Create a new sheet for individual particle information
ws_particles = wb.create_sheet('individ_img_info')

# Start writing to the second row; first row for headers
row_idx = 2

# Write the headers
ws_particles.cell(row=1, column=1, value="Image Name")
ws_particles.cell(row=1, column=2, value="Particle ID")
ws_particles.cell(row=1, column=3, value="Diameter")
ws_particles.cell(row=1, column=4, value="Area")

# Iterate through each text file and write the particle data to the new sheet
for filename in os.listdir(postprocessed_imgs_path):
    if filename.startswith('boundingboxes_') and filename.endswith('.txt'):
        file_path = os.path.join(postprocessed_imgs_path, filename)
        image_name = filename.replace('boundingboxes_', '').replace('.txt', '')
        particle_data = parse_particle_data(file_path)

        # Write image name and particle data
        for particle_id, diameter, area in particle_data:
            ws_particles.cell(row=row_idx, column=1, value=image_name)
            ws_particles.cell(row=row_idx, column=2, value=particle_id)
            ws_particles.cell(row=row_idx, column=3, value=diameter)
            ws_particles.cell(row=row_idx, column=4, value=area)
            row_idx += 1

        # Leave an empty row after each image's data
        row_idx += 1

# Save the workbook with the new sheet
wb.save(output_excel_path)

# The path to the Excel file is returned for your reference.
output_excel_path

"""# **Export final data products**"""

# Ensure the imgs directory exists
if not os.path.exists(imgs_output_dir):
    os.makedirs(imgs_output_dir)

# Copy the annotated images and the cumulative distribution image to the imgs output directory
for filename in os.listdir(postprocessing_imgs_path):
    if filename.endswith('_annotated.jpg') or filename == 'cumulative_diameter_distribution.png':
        shutil.copy(os.path.join(postprocessing_imgs_path, filename), imgs_output_dir)

# Zip the contents of the '/content/data/output' folder
with ZipFile(zip_file_path, 'w') as zipf:
    for root, dirs, files in os.walk(output_folder):
        for file in files:
            zipf.write(os.path.join(root, file),
                       os.path.relpath(os.path.join(root, file),
                                       os.path.join(output_folder, '..')))

zip_location = os.path.join(main_folder_path,'output.zip')
# files.download(zip_location) # Need help from Sarah DevOps to export this zip